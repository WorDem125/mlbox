{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e25b61f",
   "metadata": {},
   "source": [
    "### 3.1 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "–†–∞–∑—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ —É—Ä–æ–≤–Ω—é —Ä–∏—Å–∫–∞ (–Ω–∏–∑–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, –≤—ã—Å–æ–∫–∏–π) –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ê–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º: —Ç–æ—á–Ω–æ—Å—Ç—å, –ø–æ–ª–Ω–æ—Ç–∞, F1-–º–µ—Ä–∞, ROC-AUC. –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å, —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –µ—ë –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95468345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded\n",
      "‚û° Rows: 2,000,000 | Columns: 12\n",
      "üìå Columns: customer_id, tr_datetime, mcc_code, tr_type, amount, hour, flow, rule_score, anomaly_score, risk_score, risk_level, verification_complexity\n",
      "\n",
      "üìÑ Preview (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tr_datetime</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>tr_type</th>\n",
       "      <th>amount</th>\n",
       "      <th>hour</th>\n",
       "      <th>flow</th>\n",
       "      <th>rule_score</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>verification_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39026145</td>\n",
       "      <td>0 10:23:26</td>\n",
       "      <td>4814</td>\n",
       "      <td>1030</td>\n",
       "      <td>-2245.92</td>\n",
       "      <td>10</td>\n",
       "      <td>spend</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.765518</td>\n",
       "      <td>13.906207</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39026145</td>\n",
       "      <td>1 10:19:29</td>\n",
       "      <td>6011</td>\n",
       "      <td>7010</td>\n",
       "      <td>56147.89</td>\n",
       "      <td>10</td>\n",
       "      <td>income</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.539441</td>\n",
       "      <td>44.615776</td>\n",
       "      <td>medium</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39026145</td>\n",
       "      <td>1 10:20:56</td>\n",
       "      <td>4829</td>\n",
       "      <td>2330</td>\n",
       "      <td>-56147.89</td>\n",
       "      <td>10</td>\n",
       "      <td>spend</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.539441</td>\n",
       "      <td>44.615776</td>\n",
       "      <td>medium</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id tr_datetime  mcc_code  tr_type    amount  hour    flow  \\\n",
       "0     39026145  0 10:23:26      4814     1030  -2245.92    10   spend   \n",
       "1     39026145  1 10:19:29      6011     7010  56147.89    10  income   \n",
       "2     39026145  1 10:20:56      4829     2330 -56147.89    10   spend   \n",
       "\n",
       "   rule_score  anomaly_score  risk_score risk_level verification_complexity  \n",
       "0        20.0       4.765518   13.906207        low                  medium  \n",
       "1        70.0       6.539441   44.615776     medium                  simple  \n",
       "2        70.0       6.539441   44.615776     medium                  simple  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Target columns found: ['risk_level', 'verification_complexity']\n",
      "\n",
      "‚ö° Sampled to 200,000 rows for training\n",
      "\n",
      "[OK] Features: 7 | num: 5 | cat: 2\n",
      "[OK] train: (160000, 7) | test: (40000, 7)\n",
      "\n",
      "=== TRAIN: risk_level ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iriska/Desktop/transaction_risk_system/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[risk_level] LogReg: f1_macro=0.4521 acc=0.6516\n",
      "[risk_level] RandomForest: f1_macro=0.8912 acc=0.9555\n",
      "[risk_level] HistGB: f1_macro=0.8989 acc=0.9569\n",
      "\n",
      "=== TRAIN: verification_complexity ===\n",
      "[verification_complexity] LogReg: f1_macro=0.4359 acc=0.7177\n",
      "[verification_complexity] RandomForest: f1_macro=0.6412 acc=0.9748\n",
      "[verification_complexity] HistGB: f1_macro=0.6386 acc=0.9709\n",
      "\n",
      "=== RESULTS TABLE: risk_level ===\n",
      " accuracy  recall_macro  f1_macro  roc_auc_ovr        model     target\n",
      " 0.956900      0.880467  0.898898     0.990947       HistGB risk_level\n",
      " 0.955475      0.864161  0.891171     0.989360 RandomForest risk_level\n",
      " 0.651575      0.498787  0.452071     0.769140       LogReg risk_level\n",
      "\n",
      "=== RESULTS TABLE: verification_complexity ===\n",
      " accuracy  recall_macro  f1_macro  roc_auc_ovr        model                  target\n",
      " 0.974775      0.633847  0.641170     0.971027 RandomForest verification_complexity\n",
      " 0.970875      0.631654  0.638587     0.705348       HistGB verification_complexity\n",
      " 0.717700      0.467440  0.435934     0.851992       LogReg verification_complexity\n",
      "\n",
      "‚úÖ Best risk model: HistGB\n",
      "‚úÖ Best complexity model: RandomForest\n",
      "\n",
      "--- BEST RISK MODEL REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hard       0.00      0.00      0.00         0\n",
      "        high       0.00      0.00      0.00      1871\n",
      "         low       0.00      0.00      0.00     30807\n",
      "      medium       0.04      0.19      0.07      7322\n",
      "      simple       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.04     40000\n",
      "   macro avg       0.01      0.04      0.01     40000\n",
      "weighted avg       0.01      0.04      0.01     40000\n",
      "\n",
      "\n",
      "--- BEST COMPLEXITY MODEL REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hard       0.00      0.00      0.00         7\n",
      "      medium       0.98      0.99      0.98     31443\n",
      "      simple       0.97      0.91      0.94      8550\n",
      "\n",
      "    accuracy                           0.97     40000\n",
      "   macro avg       0.65      0.63      0.64     40000\n",
      "weighted avg       0.97      0.97      0.97     40000\n",
      "\n",
      "\n",
      "[SAVED]\n",
      " - ../models/best_model_risk.joblib\n",
      " - ../models/best_model_complexity.joblib\n",
      " - ../models/metrics_31.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MODULE C / 3.1 ‚Äî TRAINING AGENT (CLEAN UNIVERSAL TEMPLATE)\n",
    "# =========================\n",
    "# –ß—Ç–æ –¥–µ–ª–∞–µ—Ç:\n",
    "# - –≥—Ä—É–∑–∏—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É transactions_labeled –∏–∑ SQLite\n",
    "# - –≥–æ—Ç–æ–≤–∏—Ç X/y –¥–ª—è –¥–≤—É—Ö —Ü–µ–ª–µ–π:\n",
    "#   1) risk_level (low/medium/high)\n",
    "#   2) verification_complexity (simple/medium/hard)\n",
    "# - –ø—Ä–æ–±—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π, —Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –ø–æ F1_macro\n",
    "# - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 2 –º–æ–¥–µ–ª–∏ + metrics_31.csv\n",
    "#\n",
    "# –ü–æ—á–µ–º—É 2 –º–æ–¥–µ–ª–∏?\n",
    "# –ü–æ—Ç–æ–º—É —á—Ç–æ 2 —Ä–∞–∑–Ω—ã–µ —Ü–µ–ª–∏ (risk –∏ complexity) -> –¥–≤–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) SETTINGS ‚Äî –æ–±—ã—á–Ω–æ –º–µ–Ω—è–µ—à—å —Ç–æ–ª—å–∫–æ —ç—Ç–æ –Ω–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö\n",
    "# ============================================================\n",
    "\n",
    "DB_PATH = \"../db/app.db\"                 # <-- –ø–æ–º–µ–Ω—è–π –ø—É—Ç—å, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—à—å –∏–∑ –¥—Ä—É–≥–æ–π –ø–∞–ø–∫–∏\n",
    "LABELED_TABLE = \"transactions_labeled\"   # <-- –∏–º—è —Ç–∞–±–ª–∏—Ü—ã –∏–∑ 2.3\n",
    "\n",
    "TARGET_RISK = \"risk_level\"                 # <-- –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–ª–∏ ‚Ññ1\n",
    "TARGET_COMPLEX = \"verification_complexity\" # <-- –∫–æ–ª–æ–Ω–∫–∞ —Ü–µ–ª–∏ ‚Ññ2\n",
    "\n",
    "MODEL_DIR = \"../models\"                  # <-- –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–Ω–∞ MacBook Air –ª—É—á—à–µ 100k‚Äì300k; –Ω–∞ –º–æ—â–Ω–æ–º –ü–ö –º–æ–∂–Ω–æ 500k‚Äì2M)\n",
    "MAX_TRAIN_ROWS = 200_000   # <-- None –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç\n",
    "\n",
    "# ‚ö†Ô∏è –ó–∞—â–∏—Ç–∞ –æ—Ç leakage:\n",
    "# –ï—Å–ª–∏ –º–µ—Ç–∫–∏ (risk_level) —Å–¥–µ–ª–∞–Ω—ã –∏–∑ anomaly_score/rule_score/risk_score,\n",
    "# —Ç–æ —ç—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ –ù–ï–õ–¨–ó–Ø –æ—Å—Ç–∞–≤–ª—è—Ç—å –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö ‚Äî –∏–Ω–∞—á–µ –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥—É—Ç \"–∏–¥–µ–∞–ª—å–Ω—ã–µ\", –Ω–æ –Ω–µ—á–µ—Å—Ç–Ω—ã–µ.\n",
    "DROP_COLS = [\n",
    "    TARGET_RISK,\n",
    "    TARGET_COMPLEX,\n",
    "\n",
    "    # --- –∞–Ω—Ç–∏-leakage (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ) ---\n",
    "    \"rule_score\",\n",
    "    \"anomaly_score\",\n",
    "    \"risk_score\",\n",
    "\n",
    "    # --- —á–∞—Å—Ç–æ –µ—â—ë —É–±–∏—Ä–∞—é—Ç —Å—Ç—Ä–æ–∫–æ–≤—É—é –¥–∞—Ç—É, –µ—Å–ª–∏ –æ–Ω–∞ –º–µ—à–∞–µ—Ç ---\n",
    "    # \"tr_datetime\",\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD DATA (SQLite)\n",
    "# ============================================================\n",
    "\n",
    "def load_table_sqlite(db_path: str, table: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"DB not found: {db_path}\")\n",
    "\n",
    "    con = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        df_ = pd.read_sql(f\"SELECT * FROM {table}\", con)\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    return df_\n",
    "\n",
    "df = load_table_sqlite(DB_PATH, LABELED_TABLE)\n",
    "\n",
    "print(\"‚úÖ Dataset loaded\")\n",
    "print(f\"‚û° Rows: {df.shape[0]:,} | Columns: {df.shape[1]}\")\n",
    "print(\"üìå Columns:\", \", \".join(df.columns))\n",
    "print(\"\\nüìÑ Preview (first 3 rows):\")\n",
    "display(df.head(3))\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ü–µ–ª–µ–π\n",
    "missing_targets = [c for c in [TARGET_RISK, TARGET_COMPLEX] if c not in df.columns]\n",
    "if missing_targets:\n",
    "    raise ValueError(f\"‚ùå Missing target columns: {missing_targets}\")\n",
    "print(\"\\n‚úÖ Target columns found:\", [TARGET_RISK, TARGET_COMPLEX])\n",
    "\n",
    "# –ë–∞–∑–æ–≤–∞—è —á–∏—Å—Ç–∫–∞\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# –°—ç–º–ø–ª (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "if MAX_TRAIN_ROWS is not None and len(df) > int(MAX_TRAIN_ROWS):\n",
    "    df = df.sample(n=int(MAX_TRAIN_ROWS), random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    print(f\"\\n‚ö° Sampled to {len(df):,} rows for training\")\n",
    "else:\n",
    "    print(\"\\n‚ö° Sampling not required\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) BUILD X, y\n",
    "# ============================================================\n",
    "\n",
    "# –ü—Ä–∏–∑–Ω–∞–∫–∏ = –≤—Å—ë, –∫—Ä–æ–º–µ DROP_COLS\n",
    "feature_cols = [c for c in df.columns if c not in DROP_COLS]\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "y_risk = df[TARGET_RISK].astype(str)\n",
    "y_cx = df[TARGET_COMPLEX].astype(str)\n",
    "\n",
    "# –¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"int32\", \"float64\", \"float32\"]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"\\n[OK] Features: {len(feature_cols)} | num: {len(num_cols)} | cat: {len(cat_cols)}\")\n",
    "\n",
    "# Train/test split (—Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ risk)\n",
    "X_train, X_test, y_risk_train, y_risk_test, y_cx_train, y_cx_test = train_test_split(\n",
    "    X, y_risk, y_cx,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_risk\n",
    ")\n",
    "print(f\"[OK] train: {X_train.shape} | test: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) PREPROCESSING (–≥–ª–∞–≤–Ω—ã–π —Ñ–∏–∫—Å sparse/dense)\n",
    "# ============================================================\n",
    "\n",
    "# A) OneHot -> —á–∞—Å—Ç–æ sparse -> –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "preprocess_onehot = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler(with_mean=False))  # safe with sparse\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# B) Ordinal -> dense -> –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–µ—Ä–µ–≤—å—è–º/HistGB\n",
    "preprocess_ordinal = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ord\", OrdinalEncoder(\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=-1\n",
    "            ))\n",
    "        ]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) MODELS (–±—ã—Å—Ç—Ä–æ + —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)\n",
    "# ============================================================\n",
    "\n",
    "MODELS = {\n",
    "    \"LogReg\": (\"onehot\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )),\n",
    "\n",
    "    \"RandomForest\": (\"ordinal\", RandomForestClassifier(\n",
    "        n_estimators=250,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    )),\n",
    "\n",
    "    \"HistGB\": (\"ordinal\", HistGradientBoostingClassifier(\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) METRICS\n",
    "# ============================================================\n",
    "\n",
    "def eval_multiclass(y_true, y_pred, y_proba=None):\n",
    "    out = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"roc_auc_ovr\": np.nan\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            out[\"roc_auc_ovr\"] = roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_and_select(Xtr, ytr, Xte, yte, target_name: str):\n",
    "    rows = []\n",
    "    best_name, best_pipe, best_f1 = None, None, -1\n",
    "\n",
    "    for name, (prep_kind, clf) in MODELS.items():\n",
    "        preprocess = preprocess_onehot if prep_kind == \"onehot\" else preprocess_ordinal\n",
    "\n",
    "        pipe = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", clf)\n",
    "        ])\n",
    "\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        pred = pipe.predict(Xte)\n",
    "\n",
    "        proba = None\n",
    "        if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "            try:\n",
    "                proba = pipe.predict_proba(Xte)\n",
    "            except Exception:\n",
    "                proba = None\n",
    "\n",
    "        m = eval_multiclass(yte, pred, proba)\n",
    "        m[\"model\"] = name\n",
    "        m[\"target\"] = target_name\n",
    "        rows.append(m)\n",
    "\n",
    "        print(f\"[{target_name}] {name}: f1_macro={m['f1_macro']:.4f} acc={m['accuracy']:.4f}\")\n",
    "\n",
    "        if m[\"f1_macro\"] > best_f1:\n",
    "            best_f1 = m[\"f1_macro\"]\n",
    "            best_name = name\n",
    "            best_pipe = pipe\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "    return res, best_name, best_pipe\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) TRAIN for BOTH targets\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== TRAIN: risk_level ===\")\n",
    "res_risk, best_risk_name, best_risk_pipe = train_and_select(\n",
    "    X_train, y_risk_train, X_test, y_risk_test, \"risk_level\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRAIN: verification_complexity ===\")\n",
    "res_cx, best_cx_name, best_cx_pipe = train_and_select(\n",
    "    X_train, y_cx_train, X_test, y_cx_test, \"verification_complexity\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== RESULTS TABLE: risk_level ===\")\n",
    "print(res_risk.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== RESULTS TABLE: verification_complexity ===\")\n",
    "print(res_cx.to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úÖ Best risk model: {best_risk_name}\")\n",
    "print(f\"‚úÖ Best complexity model: {best_cx_name}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) REPORTS (—á—Ç–æ–±—ã –Ω–∞ –∑–∞—â–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç—å)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- BEST RISK MODEL REPORT ---\")\n",
    "risk_pred = best_risk_pipe.predict(X_test)\n",
    "print(classification_report(y_risk_test, risk_pred, zero_division=0))\n",
    "\n",
    "print(\"\\n--- BEST COMPLEXITY MODEL REPORT ---\")\n",
    "cx_pred = best_cx_pipe.predict(X_test)\n",
    "print(classification_report(y_cx_test, cx_pred, zero_division=0))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) SAVE MODELS + METRICS\n",
    "# ============================================================\n",
    "\n",
    "risk_path = os.path.join(MODEL_DIR, \"best_model_risk.joblib\")\n",
    "cx_path = os.path.join(MODEL_DIR, \"best_model_complexity.joblib\")\n",
    "metrics_path = os.path.join(MODEL_DIR, \"metrics_31.csv\")\n",
    "\n",
    "joblib.dump(best_risk_pipe, risk_path)\n",
    "joblib.dump(best_cx_pipe, cx_path)\n",
    "\n",
    "pd.concat([res_risk, res_cx], ignore_index=True).to_csv(metrics_path, index=False)\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(\" -\", risk_path)\n",
    "print(\" -\", cx_path)\n",
    "print(\" -\", metrics_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
